{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cs4oOlNmG5kV"
   },
   "source": [
    "# Object Localization Notebook\n",
    "#### Author: Tyler Newton  \n",
    "  \n",
    "### Implement a simple object detector for volcanics dataset.  \n",
    "#### Goals:\n",
    "- Use [AlpacaDB implementation of selective search](https://github.com/AlpacaDB/selectivesearch) to generate region proposals.\n",
    "- Crop region proposals from input images and classify with pretrained model.  \n",
    "- Perform [non-maximum supression](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.png\n"
     ]
    }
   ],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # Remove background from images # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# This doesn't work well on particles images. \n",
    "\n",
    "from rembg.bg import remove\n",
    "# import rembg\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import os\n",
    "\n",
    "# specify image directories\n",
    "ROOT = \"/Users/human/Dropbox/Research/ML_image_classification/notebooks/particles\"\n",
    "images_dir = os.path.join(ROOT, 'originals')\n",
    "output_dir = os.path.join(ROOT, 'background_removed')\n",
    "\n",
    "# walk the image directories \n",
    "for _, _, files in os.walk(images_dir):\n",
    "    # loop through files and skip . files\n",
    "    for file in files:\n",
    "        if file[0] != \".\":\n",
    "            print(file)\n",
    "            # define filepaths for input and output\n",
    "            input_path = os.path.join(images_dir, file)\n",
    "            output_path = os.path.join(output_dir, file)\n",
    "\n",
    "            # remove the background and save to new file\n",
    "            f = np.fromfile(input_path)\n",
    "            # parameters are manually tuned to work across classes\n",
    "            result = remove(f, alpha_matting=True, alpha_matting_foreground_threshold=0.0001, alpha_matting_background_threshold=0.00000001, alpha_matting_base_size=100000000)\n",
    "            img = Image.open(io.BytesIO(result)).convert(\"RGB\")\n",
    "            img.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # Implement Selective Search # # # # # # # # # # # # # # \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "import selectivesearch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# store results in dict\n",
    "ss_dict = {}\n",
    "\n",
    "# loop through all images in trainset\n",
    "for index, image in enumerate(images):\n",
    "    # get current image\n",
    "    img = images[index].numpy()\n",
    "    \n",
    "    # apply selective search to image\n",
    "    img_lbl, regions = selectivesearch.selective_search(img, scale=500, sigma=0.9, min_size=10)\n",
    "\n",
    "    # narrow down selective search results\n",
    "    candidates = set()\n",
    "    for r in regions:\n",
    "        # excluding same rectangle (with different segments)\n",
    "        if r['rect'] in candidates:\n",
    "            continue\n",
    "        # excluding regions smaller than 2000 pixels\n",
    "        if r['size'] < 500:\n",
    "            continue\n",
    "        # distorted rects\n",
    "        x, y, w, h = r['rect']\n",
    "        if w / h > 1.2 or h / w > 1.2:\n",
    "            continue\n",
    "        candidates.add(r['rect'])\n",
    "\n",
    "    # # draw rectangles on the original image to plot region proposals\n",
    "    # fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))\n",
    "    # ax.imshow(img)\n",
    "    # print(\"x, y, w, h\")\n",
    "    # for x, y, w, h in candidates:\n",
    "    #     print(x, y, w, h)\n",
    "    #     rect = mpatches.Rectangle(\n",
    "    #         (x, y), w, h, fill=False, edgecolor='red', linewidth=1)\n",
    "    #     ax.add_patch(rect)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    # store image, label, and region proposals in dict\n",
    "    ss_dict[index] = [img, labels[index], candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save/load selective search results\n",
    "import pickle\n",
    "\n",
    "# # save processed selective search region proposals as pickle file\n",
    "# outfile = open('ss_dict.pkl', 'wb')\n",
    "# pickle.dump(ss_dict, outfile)\n",
    "# outfile.close()\n",
    "\n",
    "# load region proposals from pickle file\n",
    "infile = open('ss_dict.pkl', 'rb')\n",
    "ss_dict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # Load a model for classification # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# use deeplabv3_resnet101 from https://pytorch.org/vision/stable/models.html\n",
    "import torchvision.models as models\n",
    "# from torch.hub import load_state_dict_from_url\n",
    "import torch\n",
    "\n",
    "# assigns GPU as device if available\n",
    "print(f'GPUs: {torch.cuda.device_count()}')\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(dev))\n",
    "\n",
    "# import untrained model adapted for 10 classes and send it to the relevant device\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True).to(dev)\n",
    "# model = torch.hub.load('pytorch/vision:v0.9.0', 'deeplabv3_resnet101', pretrained=True).to(dev)\n",
    "# model.eval()\n",
    "\n",
    "print(f\"Model parameters: {len(list(model.parameters()))}\")\n",
    "print(f\"Model architecture: \\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # # Crop region proposals # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# store cropped regions in a dict\n",
    "crop_dict = {}\n",
    "\n",
    "# loop through all entries (images) in selective search dict\n",
    "for entry in ss_dict.keys():\n",
    "    image = ss_dict[entry][0]\n",
    "    label = ss_dict[entry][1]\n",
    "    regions = ss_dict[entry][2]\n",
    "\n",
    "    crop_dict[entry] = []\n",
    "    for region in regions:\n",
    "        # append image of cropped region from proposal\n",
    "        x1 = region[0]\n",
    "        x2 = region[0] + region[2]\n",
    "        y1 = region[1]\n",
    "        y2 = region[1] + region[3]\n",
    "        crop_dict[entry].append(image[x1:x2, y1:y2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify cropped regions\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "label_dict = {}\n",
    "\n",
    "# make a data class to transform cropped images\n",
    "class DataTrans(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = Image.fromarray(self.data[index])\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# transform and load data into DataLoader\n",
    "for entry in crop_dict.keys():\n",
    "    print(f\"{entry} of {len(crop_dict.keys())}\")\n",
    "    data = crop_dict[entry]\n",
    "    labels = [int(ss_dict[entry][1]) for item in crop_dict[entry]]\n",
    "    if len(labels) > 0:\n",
    "        classset = DataTrans(data, labels, transform=transform)\n",
    "        class_loader = DataLoader(classset, batch_size=len(classset), shuffle=False, num_workers=0)\n",
    "\n",
    "        # get a batch of training data to preview\n",
    "        dataiter = iter(class_loader)\n",
    "        images, labels = dataiter.next() # gets all images in set\n",
    "\n",
    "        # get classification predictions from pretrained model\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output = model(images)['out']\n",
    "\n",
    "            # store classification predictions for each cropped image\n",
    "            label_dict[entry] = [[], []] # [[actual labels], [predicted labels]\n",
    "            for index, label in enumerate(labels):\n",
    "                prediction = int(torch.unique(output[index].argmax(0)).max())\n",
    "                label_dict[entry][0].append(int(label))\n",
    "                label_dict[entry][1].append(prediction)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save/load classification results\n",
    "\n",
    "# save classifications as pickle file\n",
    "outfile = open('label_dict.pkl', 'wb')\n",
    "pickle.dump(label_dict, outfile)\n",
    "outfile.close()\n",
    "\n",
    "# # load classifications from pickle file\n",
    "# infile = open('label_dict.pkl', 'rb')\n",
    "# label_dict = pickle.load(infile)\n",
    "# infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # Non maximum supression # # # # # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# nms implementation from: https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/\n",
    "\n",
    "# function to perform non max supression on a single image with multiple bounding boxes\n",
    "def nms(bounding_boxes, threshold):\n",
    "    \n",
    "    if len(bounding_boxes) > 0:\n",
    "        # list to store selected boxes\n",
    "        selected_boxes = []\n",
    "        # extract x's and y's\n",
    "        x1 = bounding_boxes[:, 0]\n",
    "        x2 = bounding_boxes[:, 1]\n",
    "        y1 = bounding_boxes[:, 2]\n",
    "        y2 = bounding_boxes[:, 3]\n",
    "        \n",
    "        # get area of bounding boxes\n",
    "        area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "        indices = np.argsort(y2)\n",
    "        \n",
    "        while len(indices) > 0:\n",
    "            last = len(indices) - 1\n",
    "            i = indices[last]\n",
    "            selected_boxes.append(i)\n",
    "            suppress = [last]\n",
    "            for pos in range(0, last):\n",
    "                j = indices[pos]\n",
    "                xx1 = max(x1[i], x1[j])\n",
    "                yy1 = max(y1[i], y1[j])\n",
    "                xx2 = min(x2[i], x2[j])\n",
    "                yy2 = min(y2[i], y2[j])\n",
    "                \n",
    "                w = max(0, xx2 - xx1 + 1)\n",
    "                h = max(0, yy2 - yy1 + 1)\n",
    "                overlap = float(w*h) / area[j]\n",
    "                \n",
    "                if overlap > threshold:\n",
    "                    suppress.append(pos)\n",
    "                    \n",
    "            indices = np.delete(indices, suppress)\n",
    "            \n",
    "        return bounding_boxes[selected_boxes]\n",
    "\n",
    "nms_dict = {}\n",
    "# loop through all entries in selective search dict\n",
    "for entry in ss_dict.keys():\n",
    "    image = ss_dict[entry][0]\n",
    "    label = ss_dict[entry][1]\n",
    "    regions = ss_dict[entry][2]\n",
    "    region_list = []\n",
    "\n",
    "    crop_dict[entry] = []\n",
    "    for region in regions:\n",
    "        # append image of cropped region from proposal\n",
    "        x1 = region[0]\n",
    "        x2 = region[0] + region[2]\n",
    "        y1 = region[1]\n",
    "        y2 = region[1] + region[3]\n",
    "        region_list.append([x1, x2, y1, y2])\n",
    "        \n",
    "    region_array = np.array(region_list)\n",
    "    nms_boxes = nms(region_array, 0.5)\n",
    "    nms_dict[entry] = nms_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_HW1_TylerNewton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
